{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d97e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./846/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: pyarrow in ./846/lib/python3.13/site-packages (21.0.0)\n",
      "Requirement already satisfied: tqdm in ./846/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: regex in ./846/lib/python3.13/site-packages (2025.9.18)\n",
      "Requirement already satisfied: seaborn in ./846/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in ./846/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./846/lib/python3.13/site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./846/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./846/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./846/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./846/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./846/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./846/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./846/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./846/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./846/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./846/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in ./846/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Directories ready:\n",
      "/Users/amaanahmed259/Downloads/16919051 /Users/amaanahmed259/Downloads/16919051/parquet /Users/amaanahmed259/Downloads/16919051/results2\n"
     ]
    }
   ],
   "source": [
    "# env setup\n",
    "!pip install pandas pyarrow tqdm regex seaborn matplotlib\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"parquet\"\n",
    "RESULTS_DIR = BASE_DIR / \"results2\"\n",
    "CONTEXT_DIR = RESULTS_DIR / \"contexts2\"\n",
    "LLM_DIR = RESULTS_DIR / \"llm_results2\"\n",
    "\n",
    "for d in [RESULTS_DIR, CONTEXT_DIR, LLM_DIR]:\n",
    "    d.mkdir(exist_ok = True)\n",
    "\n",
    "print(\"Directories ready:\")\n",
    "print(BASE_DIR, DATA_DIR, RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b01ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded pull_request: 33,596 rows\n",
      "‚úÖ Loaded pr_timeline: 325,500 rows\n",
      "‚úÖ Loaded pr_reviews: 28,875 rows\n",
      "‚úÖ Loaded pr_review_comments: 19,450 rows\n",
      "‚úÖ Loaded pr_commits: 88,576 rows\n",
      "‚úÖ Loaded pr_commit_details: 711,923 rows\n",
      "‚úÖ Loaded repository: 2,807 rows\n",
      "‚úÖ Loaded pr_comments: 39,122 rows\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "\n",
    "tables = {}\n",
    "for name in [\n",
    "    \"pull_request\", \"pr_timeline\", \"pr_reviews\", \"pr_review_comments\",\n",
    "    \"pr_commits\", \"pr_commit_details\", \"repository\", \"pr_comments\"\n",
    "]:\n",
    "    path = DATA_DIR / f\"{name}.parquet\"\n",
    "    if path.exists():\n",
    "        tables[name] = pd.read_parquet(path)\n",
    "        print(f\"‚úÖ Loaded {name}: {len(tables[name]):,} rows\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing {name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed & not merged: 7270\n",
      "Stale open (treated as failures): 365\n",
      "Total failed PRs: 7635\n",
      "Saved: /Users/amaanahmed259/Downloads/16919051/results2/failed_prs_refined.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>agent</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user</th>\n",
       "      <th>state</th>\n",
       "      <th>created_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>merged_at</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>state_lower</th>\n",
       "      <th>outcome</th>\n",
       "      <th>last_activity</th>\n",
       "      <th>ci_failed</th>\n",
       "      <th>changes_requested</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3264933329</td>\n",
       "      <td>2911</td>\n",
       "      <td>Fix: Wait for all partitions in load_collectio...</td>\n",
       "      <td>## Summary\\n\\nFixes an issue where `load_colle...</td>\n",
       "      <td>Claude_Code</td>\n",
       "      <td>108661493</td>\n",
       "      <td>weiliu1031</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-07-26 02:59:01+00:00</td>\n",
       "      <td>2025-07-29 07:01:20+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>191751505</td>\n",
       "      <td>https://api.github.com/repos/milvus-io/pymilvus</td>\n",
       "      <td>https://github.com/milvus-io/pymilvus/pull/2911</td>\n",
       "      <td>closed</td>\n",
       "      <td>closed_unmerged</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3234102722</td>\n",
       "      <td>318</td>\n",
       "      <td>chore: Convert hive-mind coordination system t...</td>\n",
       "      <td>## Summary\\n\\nThis PR converts the AI agent co...</td>\n",
       "      <td>Claude_Code</td>\n",
       "      <td>15803865</td>\n",
       "      <td>lanemc</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-07-16 01:00:34+00:00</td>\n",
       "      <td>2025-07-17 12:49:29+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>995029641</td>\n",
       "      <td>https://api.github.com/repos/ruvnet/claude-flow</td>\n",
       "      <td>https://github.com/ruvnet/claude-flow/pull/318</td>\n",
       "      <td>closed</td>\n",
       "      <td>closed_unmerged</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3212961701</td>\n",
       "      <td>113</td>\n",
       "      <td>fix: Add missing logger.js and resolve MCP ser...</td>\n",
       "      <td>## Summary\\n\\nFixed critical MCP server issues...</td>\n",
       "      <td>Claude_Code</td>\n",
       "      <td>2934394</td>\n",
       "      <td>ruvnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-07-08 15:26:38+00:00</td>\n",
       "      <td>2025-07-08 15:27:58+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1009254201</td>\n",
       "      <td>https://api.github.com/repos/ruvnet/ruv-FANN</td>\n",
       "      <td>https://github.com/ruvnet/ruv-FANN/pull/113</td>\n",
       "      <td>closed</td>\n",
       "      <td>closed_unmerged</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164503419</td>\n",
       "      <td>40</td>\n",
       "      <td>Fix Claude animation flickering with vt10x-ins...</td>\n",
       "      <td>## üéØ Problem: Claude's Thinking Animation Caus...</td>\n",
       "      <td>Claude_Code</td>\n",
       "      <td>2891702</td>\n",
       "      <td>hjanuschka</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-06-20 22:47:18+00:00</td>\n",
       "      <td>2025-06-21 11:51:22+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1002552148</td>\n",
       "      <td>https://api.github.com/repos/amantus-ai/vibetu...</td>\n",
       "      <td>https://github.com/amantus-ai/vibetunnel/pull/40</td>\n",
       "      <td>closed</td>\n",
       "      <td>closed_unmerged</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3165440543</td>\n",
       "      <td>1030</td>\n",
       "      <td>Fix agents page module resolution error and up...</td>\n",
       "      <td>## Summary\\n- Fixed critical module resolution...</td>\n",
       "      <td>Claude_Code</td>\n",
       "      <td>14167547</td>\n",
       "      <td>AtlantisPleb</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-06-21 21:22:44+00:00</td>\n",
       "      <td>2025-06-21 22:53:43+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>715683924</td>\n",
       "      <td>https://api.github.com/repos/OpenAgentsInc/ope...</td>\n",
       "      <td>https://github.com/OpenAgentsInc/openagents/pu...</td>\n",
       "      <td>closed</td>\n",
       "      <td>closed_unmerged</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  number                                              title  \\\n",
       "0  3264933329    2911  Fix: Wait for all partitions in load_collectio...   \n",
       "1  3234102722     318  chore: Convert hive-mind coordination system t...   \n",
       "2  3212961701     113  fix: Add missing logger.js and resolve MCP ser...   \n",
       "3  3164503419      40  Fix Claude animation flickering with vt10x-ins...   \n",
       "4  3165440543    1030  Fix agents page module resolution error and up...   \n",
       "\n",
       "                                                body        agent    user_id  \\\n",
       "0  ## Summary\\n\\nFixes an issue where `load_colle...  Claude_Code  108661493   \n",
       "1  ## Summary\\n\\nThis PR converts the AI agent co...  Claude_Code   15803865   \n",
       "2  ## Summary\\n\\nFixed critical MCP server issues...  Claude_Code    2934394   \n",
       "3  ## üéØ Problem: Claude's Thinking Animation Caus...  Claude_Code    2891702   \n",
       "4  ## Summary\\n- Fixed critical module resolution...  Claude_Code   14167547   \n",
       "\n",
       "           user   state                created_at                 closed_at  \\\n",
       "0    weiliu1031  closed 2025-07-26 02:59:01+00:00 2025-07-29 07:01:20+00:00   \n",
       "1        lanemc  closed 2025-07-16 01:00:34+00:00 2025-07-17 12:49:29+00:00   \n",
       "2        ruvnet  closed 2025-07-08 15:26:38+00:00 2025-07-08 15:27:58+00:00   \n",
       "3    hjanuschka  closed 2025-06-20 22:47:18+00:00 2025-06-21 11:51:22+00:00   \n",
       "4  AtlantisPleb  closed 2025-06-21 21:22:44+00:00 2025-06-21 22:53:43+00:00   \n",
       "\n",
       "  merged_at     repo_id                                           repo_url  \\\n",
       "0       NaT   191751505    https://api.github.com/repos/milvus-io/pymilvus   \n",
       "1       NaT   995029641    https://api.github.com/repos/ruvnet/claude-flow   \n",
       "2       NaT  1009254201       https://api.github.com/repos/ruvnet/ruv-FANN   \n",
       "3       NaT  1002552148  https://api.github.com/repos/amantus-ai/vibetu...   \n",
       "4       NaT   715683924  https://api.github.com/repos/OpenAgentsInc/ope...   \n",
       "\n",
       "                                            html_url state_lower  \\\n",
       "0    https://github.com/milvus-io/pymilvus/pull/2911      closed   \n",
       "1     https://github.com/ruvnet/claude-flow/pull/318      closed   \n",
       "2        https://github.com/ruvnet/ruv-FANN/pull/113      closed   \n",
       "3   https://github.com/amantus-ai/vibetunnel/pull/40      closed   \n",
       "4  https://github.com/OpenAgentsInc/openagents/pu...      closed   \n",
       "\n",
       "           outcome last_activity  ci_failed  changes_requested  \n",
       "0  closed_unmerged           NaT      False              False  \n",
       "1  closed_unmerged           NaT      False              False  \n",
       "2  closed_unmerged           NaT      False              False  \n",
       "3  closed_unmerged           NaT      False              False  \n",
       "4  closed_unmerged           NaT      False              False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute refined failed_prs (closed_unmerged + stale_open)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "pull = tables[\"pull_request\"].copy()\n",
    "timeline = tables.get(\"pr_timeline\")\n",
    "reviews = tables.get(\"pr_reviews\")\n",
    "\n",
    "# ensure timestamps are proper datetimes for any columns that exist\n",
    "for col in [\"created_at\", \"updated_at\", \"closed_at\", \"merged_at\"]:\n",
    "    if col in pull.columns:\n",
    "        pull[col] = pd.to_datetime(pull[col], errors = \"coerce\", utc = True)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Base failure definition: CLOSED and NOT MERGED  => \"closed_unmerged\"\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "pull[\"state_lower\"] = pull[\"state\"].astype(str).str.lower()\n",
    "closed_unmerged_mask = (pull[\"state_lower\"] == \"closed\") & pull[\"merged_at\"].isna()\n",
    "\n",
    "closed_unmerged = pull[closed_unmerged_mask].copy()\n",
    "closed_unmerged[\"outcome\"] = \"closed_unmerged\"\n",
    "\n",
    "print(\"Closed & not merged:\", len(closed_unmerged))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Stale open PRs => \"stale_open\"\n",
    "#    We need some notion of \"last activity\":\n",
    "#    - Prefer 'updated_at' if present\n",
    "#    - Otherwise fall back to 'created_at'\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "now = pd.Timestamp(datetime.now(timezone.utc))\n",
    "stale_days = 180\n",
    "\n",
    "open_mask = pull[\"state_lower\"] == \"open\"\n",
    "\n",
    "# build a generic \"last_activity\" timestamp\n",
    "if \"updated_at\" in pull.columns:\n",
    "    pull[\"last_activity\"] = pull[\"updated_at\"].where(pull[\"updated_at\"].notna(), pull[\"created_at\"])\n",
    "else:\n",
    "    # no updated_at column, use created_at as a proxy\n",
    "    pull[\"last_activity\"] = pull[\"created_at\"]\n",
    "\n",
    "stale_open_mask = (\n",
    "    open_mask\n",
    "    & pull[\"last_activity\"].notna()\n",
    "    & ((now - pull[\"last_activity\"]) > pd.Timedelta(days = stale_days))\n",
    "    & pull[\"created_at\"].notna()\n",
    "    & ((now - pull[\"created_at\"]) > pd.Timedelta(days = stale_days))\n",
    ")\n",
    "\n",
    "stale_open = pull[stale_open_mask].copy()\n",
    "stale_open[\"outcome\"] = \"stale_open\"\n",
    "\n",
    "print(\"Stale open (treated as failures):\", len(stale_open))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Combine failure sets\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "failed_prs = pd.concat([closed_unmerged, stale_open], ignore_index = True)\n",
    "\n",
    "# make sure id is int (some operations later assume this)\n",
    "failed_prs[\"id\"] = failed_prs[\"id\"].astype(int)\n",
    "\n",
    "print(\"Total failed PRs:\", len(failed_prs))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Add CI failure and CHANGES_REQUESTED indicators AS SIGNALS\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "ci_failed_ids = set()\n",
    "if timeline is not None and {\"event\", \"pr_id\"}.issubset(timeline.columns):\n",
    "    mask = timeline[\"event\"].astype(str).str.contains(\"fail|workflow_failed|error\", case = False, na = False)\n",
    "    ci_failed_ids = set(timeline.loc[mask, \"pr_id\"].astype(int))\n",
    "\n",
    "cr_ids = set()\n",
    "if reviews is not None and {\"state\", \"pr_id\"}.issubset(reviews.columns):\n",
    "    mask = reviews[\"state\"].astype(str).str.contains(\"changes_requested\", case = False, na = False)\n",
    "    cr_ids = set(reviews.loc[mask, \"pr_id\"].astype(int))\n",
    "\n",
    "failed_prs[\"ci_failed\"] = failed_prs[\"id\"].isin(ci_failed_ids)\n",
    "failed_prs[\"changes_requested\"] = failed_prs[\"id\"].isin(cr_ids)\n",
    "\n",
    "failed_prs.to_csv(RESULTS_DIR / \"failed_prs_refined.csv\", index = False)\n",
    "print(\"Saved:\", RESULTS_DIR / \"failed_prs_refined.csv\")\n",
    "\n",
    "failed_prs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b66436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def truncate(s, max_chars=1500):\n",
    "    s = str(s or \"\")\n",
    "    return s if len(s) <= max_chars else s[:max_chars//2] + \"\\n...\\n\" + s[-max_chars//2:]\n",
    "\n",
    "def safe_str(x):\n",
    "    try: return str(x)\n",
    "    except: return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 369/7635 [00:01<00:34, 209.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# pr_review_comments.body (line-level review comments)\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (rev_comments \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rev_comments.empty\n\u001b[32m    122\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpull_request_review_id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rev_comments.columns\n\u001b[32m    123\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m reviews \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m reviews.columns\n\u001b[32m    125\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpr_id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m reviews.columns):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     rev_ids = reviews.loc[\u001b[43mreviews\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpr_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    127\u001b[39m     sub2 = rev_comments[rev_comments[\u001b[33m\"\u001b[39m\u001b[33mpull_request_review_id\u001b[39m\u001b[33m\"\u001b[39m].isin(rev_ids)]\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sub2.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/16919051/846/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/16919051/846/lib/python3.13/site-packages/pandas/core/arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/16919051/846/lib/python3.13/site-packages/pandas/core/series.py:6138\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6135\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6136\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6138\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/16919051/846/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:316\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# We don't catch tuple here bc we may be comparing e.g. MultiIndex\u001b[39;00m\n\u001b[32m    313\u001b[39m     \u001b[38;5;66;03m#  to a tuple that represents a single entry, see test_compare_tuple_strs\u001b[39;00m\n\u001b[32m    314\u001b[39m     rvalues = np.asarray(rvalues)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndarray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABCExtensionArray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lvalues) != \u001b[38;5;28mlen\u001b[39m(rvalues):\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLengths must match to compare\u001b[39m\u001b[33m\"\u001b[39m, lvalues.shape, rvalues.shape\n\u001b[32m    323\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/16919051/846/lib/python3.13/site-packages/pandas/core/dtypes/generic.py:44\u001b[39m, in \u001b[36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[39m\u001b[34m(cls, inst)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/16919051/846/lib/python3.13/site-packages/pandas/core/dtypes/generic.py:38\u001b[39m, in \u001b[36mcreate_pandas_abc_type.<locals>._check\u001b[39m\u001b[34m(inst)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check\u001b[39m(inst) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_typ\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m comp\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# build context files for each failed PR (using refined failed_prs)\n",
    "from tqdm import tqdm\n",
    "\n",
    "commit_details = tables.get(\"pr_commit_details\")\n",
    "repos          = tables.get(\"repository\")\n",
    "timeline       = tables.get(\"pr_timeline\")\n",
    "reviews        = tables.get(\"pr_reviews\")\n",
    "rev_comments   = tables.get(\"pr_review_comments\")\n",
    "pr_comments    = tables.get(\"pr_comments\")\n",
    "pr_commits     = tables.get(\"pr_commits\")\n",
    "\n",
    "contexts = []\n",
    "\n",
    "MAX_CONTEXTS = None  # e.g., 500 for debugging, None for full dataset\n",
    "\n",
    "if MAX_CONTEXTS is not None:\n",
    "    iter_df = failed_prs.head(MAX_CONTEXTS)\n",
    "else:\n",
    "    iter_df = failed_prs\n",
    "\n",
    "for _, pr in tqdm(iter_df.iterrows(), total=len(iter_df)):\n",
    "    pid = int(pr[\"id\"])\n",
    "    repo_id = pr.get(\"repo_id\", None)\n",
    "\n",
    "    # repository metadata\n",
    "    repo_name = \"unknown\"\n",
    "    repo_lang = \"unknown\"\n",
    "    if repos is not None and not repos.empty and \"id\" in repos.columns:\n",
    "        rsub = repos[repos[\"id\"] == repo_id]\n",
    "        if not rsub.empty:\n",
    "            rrow = rsub.iloc[0]\n",
    "            repo_name = safe_str(rrow.get(\"name\", \"unknown\"))\n",
    "            repo_lang = safe_str(rrow.get(\"language\", \"unknown\"))\n",
    "\n",
    "    # author metadata (AIDev may have author_type)\n",
    "    author_login = safe_str(pr.get(\"user_login\") or pr.get(\"author_login\"))\n",
    "    author_type  = safe_str(pr.get(\"author_type\", \"unknown\"))\n",
    "\n",
    "    state     = safe_str(pr.get(\"state\"))\n",
    "    outcome   = safe_str(pr.get(\"outcome\", \"closed_unmerged\"))\n",
    "    created_at = safe_str(pr.get(\"created_at\"))\n",
    "    updated_at = safe_str(pr.get(\"updated_at\"))\n",
    "    closed_at  = safe_str(pr.get(\"closed_at\"))\n",
    "    merged_at  = safe_str(pr.get(\"merged_at\"))\n",
    "\n",
    "    ci_failed_flag         = bool(pr.get(\"ci_failed\", False))\n",
    "    changes_requested_flag = bool(pr.get(\"changes_requested\", False))\n",
    "\n",
    "    # -----------------------------\n",
    "    # Diff summary (from pr_commit_details)\n",
    "    # -----------------------------\n",
    "    \n",
    "    diff_snippet = \"\"\n",
    "    if commit_details is not None and \"pr_id\" in commit_details.columns:\n",
    "        diff_df = commit_details[commit_details[\"pr_id\"] == pid]\n",
    "        if not diff_df.empty:\n",
    "            parts = []\n",
    "            for _, row in diff_df.head(20).iterrows():\n",
    "                fname = safe_str(row.get(\"filename\"))\n",
    "                patch = truncate(row.get(\"patch\", \"\"), 400)\n",
    "                parts.append(f\"File: {fname}\\n{patch}\")\n",
    "            diff_snippet = truncate(\"\\n\\n\".join(parts), 2000)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Timeline snippet\n",
    "    # -----------------------------\n",
    "    \n",
    "    timeline_snippet = \"\"\n",
    "    if timeline is not None and \"pr_id\" in timeline.columns:\n",
    "        tl_df = timeline[timeline[\"pr_id\"] == pid].copy()\n",
    "        if \"created_at\" in tl_df.columns:\n",
    "            tl_df[\"created_at\"] = pd.to_datetime(tl_df[\"created_at\"], errors=\"coerce\", utc=True)\n",
    "            tl_df = tl_df.sort_values(\"created_at\")\n",
    "        last_events = tl_df.tail(20)\n",
    "        lines = []\n",
    "        for _, row in last_events.iterrows():\n",
    "            ts    = safe_str(row.get(\"created_at\"))\n",
    "            ev    = safe_str(row.get(\"event\"))\n",
    "            actor = safe_str(row.get(\"actor_login\") or row.get(\"actor\"))\n",
    "            details_parts = []\n",
    "            for col in [\"state\", \"label\", \"message\"]:\n",
    "                val = safe_str(row.get(col))\n",
    "                if val and val.lower() not in (\"nan\", \"none\"):\n",
    "                    details_parts.append(val)\n",
    "            details = \" | \".join(details_parts)\n",
    "            lines.append(f\"[{ts}] {actor} ‚Äî {ev}: {details}\")\n",
    "        timeline_snippet = truncate(\"\\n\".join(lines), 2000)\n",
    "\n",
    "    # -----------------------------\n",
    "    # CI / workflow logs\n",
    "    # -----------------------------\n",
    "    \n",
    "    ci_snippet = \"\"\n",
    "    if timeline is not None and \"pr_id\" in timeline.columns:\n",
    "        ci_df = timeline[timeline[\"pr_id\"] == pid]\n",
    "        ci_mask = ci_df[\"event\"].astype(str).str.contains(\"fail|error|ci|workflow|build\", case = False, na = False)\n",
    "        ci_logs = []\n",
    "        for _, row in ci_df.loc[ci_mask].iterrows():\n",
    "            parts = []\n",
    "            for field in [\"event\", \"label\", \"message\"]:\n",
    "                val = safe_str(row.get(field))\n",
    "                if val and val.lower() not in (\"nan\", \"none\"):\n",
    "                    parts.append(val)\n",
    "            joined = \" | \".join(parts)\n",
    "            if joined.strip():\n",
    "                ci_logs.append(joined)\n",
    "        ci_snippet = truncate(\"\\n\".join(ci_logs), 1500)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Review + discussion text (reviews, review comments, PR comments)\n",
    "    # -----------------------------\n",
    "    \n",
    "    review_texts = []\n",
    "\n",
    "    # pr_reviews.body (review summaries)\n",
    "    if reviews is not None and {\"pr_id\", \"body\"}.issubset(reviews.columns):\n",
    "        sub = reviews[reviews[\"pr_id\"] == pid]\n",
    "        review_texts.extend(sub[\"body\"].dropna().astype(str).tolist())\n",
    "\n",
    "    # pr_review_comments.body (line-level review comments)\n",
    "    if (rev_comments is not None\n",
    "        and not rev_comments.empty\n",
    "        and \"pull_request_review_id\" in rev_comments.columns\n",
    "        and reviews is not None\n",
    "        and \"id\" in reviews.columns\n",
    "        and \"pr_id\" in reviews.columns):\n",
    "        rev_ids = reviews.loc[reviews[\"pr_id\"] == pid, \"id\"]\n",
    "        sub2 = rev_comments[rev_comments[\"pull_request_review_id\"].isin(rev_ids)]\n",
    "        if \"body\" in sub2.columns:\n",
    "            review_texts.extend(sub2[\"body\"].dropna().astype(str).tolist())\n",
    "\n",
    "    # general PR discussion comments\n",
    "    if pr_comments is not None and {\"pr_id\", \"body\"}.issubset(pr_comments.columns):\n",
    "        sub3 = pr_comments[pr_comments[\"pr_id\"] == pid]\n",
    "        review_texts.extend(sub3[\"body\"].dropna().astype(str).tolist())\n",
    "\n",
    "    # deduplicate while preserving order\n",
    "    review_texts = list(dict.fromkeys(review_texts))\n",
    "    review_snippet = truncate(\"\\n---\\n\".join(review_texts), 2000)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Commit messages (from pr_commits)\n",
    "    # -----------------------------\n",
    "    \n",
    "    commit_snippet = \"\"\n",
    "    if pr_commits is not None and {\"pr_id\", \"message\"}.issubset(pr_commits.columns):\n",
    "        msgs = pr_commits[pr_commits[\"pr_id\"] == pid][\"message\"].dropna().astype(str).tolist()\n",
    "        commit_snippet = truncate(\"\\n\".join(msgs), 1000)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build the final context string\n",
    "    # -----------------------------\n",
    "    \n",
    "    title = safe_str(pr.get(\"title\", \"\"))\n",
    "    body  = safe_str(pr.get(\"body\", \"\"))\n",
    "\n",
    "    ctx = f\"\"\"PR #{pid}\n",
    "Repository: {repo_name} (id={repo_id}), language={repo_lang}\n",
    "Author: {author_login} (author_type={author_type})\n",
    "Outcome: {outcome} (state={state}, merged_at={merged_at})\n",
    "Created at: {created_at}\n",
    "Updated at: {updated_at}\n",
    "Closed at: {closed_at}\n",
    "\n",
    "Signals:\n",
    "- ci_failed: {ci_failed_flag}\n",
    "- changes_requested: {changes_requested_flag}\n",
    "\n",
    "Title:\n",
    "{title}\n",
    "\n",
    "Description:\n",
    "{truncate(body, 1000)}\n",
    "\n",
    "=== DIFF SUMMARY ===\n",
    "{diff_snippet or 'N/A'}\n",
    "\n",
    "=== TIMELINE (last events) ===\n",
    "{timeline_snippet or 'N/A'}\n",
    "\n",
    "=== CI / WORKFLOW LOGS ===\n",
    "{ci_snippet or 'N/A'}\n",
    "\n",
    "=== REVIEW & DISCUSSION SNIPPET ===\n",
    "{review_snippet or 'N/A'}\n",
    "\n",
    "=== COMMIT MESSAGES ===\n",
    "{commit_snippet or 'N/A'}\n",
    "\"\"\"\n",
    "\n",
    "    out_path = CONTEXT_DIR / f\"PR_{pid}.txt\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(ctx)\n",
    "\n",
    "    contexts.append({\"id\": pid, \"path\": out_path.name})\n",
    "\n",
    "print(f\"‚úÖ Saved {len(contexts)} context files to {CONTEXT_DIR}\")\n",
    "\n",
    "# save a small index of contexts for later use\n",
    "contexts_df = pd.DataFrame(contexts)\n",
    "contexts_df.to_csv(RESULTS_DIR / \"failed_pr_context_index.csv\", index=False)\n",
    "contexts_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "846",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
